{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_DB(doc):\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    joined = db.joined\n",
    "    try:\n",
    "        joined.insert_one(doc)\n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    #closing the client\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_stream(iter):\n",
    "    \n",
    "    #we merge terra and aqua as hotspots - 2 producers\n",
    "    terra_aqua_list = []\n",
    "    #one producer for climate data\n",
    "    climate_list = []\n",
    "    for each in iter:\n",
    "        data = each[1]\n",
    "        data_json = json.loads(data)\n",
    "        \n",
    "        if data_json['sender'] == 'climate':\n",
    "            climate_list.append(data_json)\n",
    "        else:\n",
    "            terra_aqua_list.append(data_json)\n",
    "    #There are four requirements in Task C.2.a. sequentially we call them ONE, TWO, THREE and FOUR\n",
    "    ##Implementing the requirement FOUR: If the streaming application has the data from only one producer (Producer 1)\n",
    "    ##it implies that there was no fire at that time and we can store the climate data into MongoDB straight away\n",
    "    if len(terra_aqua_list) == 0:\n",
    "        for doc in climate_list:\n",
    "            send_to_DB(doc)\n",
    "    #since climate is streamed every 5 seconds, we'll always have at least one in our stream batch\n",
    "    ##According to the requirement, if we have data from at least two producers, we join them based on the location and\n",
    "    ##merge them in the same data model we created in Task B\n",
    "    ###Implementing the requirement ONE and TWO:\n",
    "    \n",
    "    #if we have one data from either aqua or terra, we embed it in the climate data\n",
    "    #as our model is one-to-many embedded documents.\n",
    "    elif len(terra_aqua_list) == 1:\n",
    "        for doc in climate_list:\n",
    "            if geohash.encode(doc['data']['latitude'],doc['data']['longitude'], precision=5) == geohash.encode(terra_aqua_list[0]['data']['latitude'],terra_aqua_list[0]['data']['longitude'], precision=5):\n",
    "                doc['data']['hotspot'] = terra_aqua_list[0]\n",
    "                send_to_DB(doc)\n",
    "            else:\n",
    "                send_to_DB(doc)\n",
    "                \n",
    "    ###Implementing the requirement THREE:\n",
    "    elif len(terra_aqua_list) == 2:\n",
    "        ter_aqua_doc1 = terra_aqua_list[0]\n",
    "        ter_aqua_doc2 = terra_aqua_list[1]\n",
    "        for doc in climate_list:\n",
    "            #climate_location == aqua_location and climate_location == terra_location\n",
    "            if (geohash.encode(doc['data']['latitude'],doc['data']['longitude'], precision=5) == geohash.encode(ter_aqua_doc1['data']['latitude'],ter_aqua_doc1['data']['longitude'], precision=5) and geohash.encode(doc['data']['latitude'],doc['data']['longitude'], precision=5) == geohash.encode(ter_aqua_doc2['data']['latitude'],ter_aqua_doc2['data']['longitude'], precision=5)):\n",
    "                ter_aqua_doc_average = ter_aqua_doc1\n",
    "                ter_aqua_doc_average['data']['confidence'] = (ter_aqua_doc1['data']['confidence']+ter_aqua_doc2['data']['confidence'])/2\n",
    "                ter_aqua_doc_average['data']['surface_temperature_celcius'] = (ter_aqua_doc1['data']['surface_temperature_celcius']+ter_aqua_doc2['data']['surface_temperature_celcius'])/2\n",
    "                doc['data']['hotspot'] = ter_aqua_doc_average\n",
    "                send_to_DB(doc)\n",
    "            elif geohash.encode(doc['data']['latitude'],doc['data']['longitude'], precision=5) == geohash.encode(ter_aqua_doc1['data']['latitude'],ter_aqua_doc1['data']['longitude'], precision=5):\n",
    "                doc['data']['hotspot'] = ter_aqua_doc1\n",
    "                send_to_DB(doc)\n",
    "            elif geohash.encode(doc['data']['latitude'],doc['data']['longitude'], precision=5) == geohash.encode(ter_aqua_doc2['data']['latitude'],ter_aqua_doc2['data']['longitude'], precision=5):\n",
    "                doc['data']['hotspot'] = ter_aqua_doc1\n",
    "                send_to_DB(doc)\n",
    "            else:\n",
    "                send_to_DB(doc)\n",
    "batch_interval = 10\n",
    "topic = \"StopFire\"\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'assignment', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "kafkaStream.pprint()\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(join_stream))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(6000) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
